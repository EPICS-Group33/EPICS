{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from cv2 import imread, createCLAHE \n",
    "import cv2\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = os.path.join(\"../input/data/Lung Segmentation/CXR_png\")\n",
    "mask_path = os.path.join(\"../input/data/Lung Segmentation/\",\"masks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0fb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 704 masks but 800 images.\n",
    "mages = os.listdir(image_path)\n",
    "mask = os.listdir(mask_path)\n",
    "mask = [fName.split(\".png\")[0] for fName in mask]\n",
    "image_file_name = [fName.split(\"_mask\")[0] for fName in mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = [i for i in mask if \"mask\" in i]\n",
    "print(\"Total mask that has modified name:\",len(check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_files = set(os.listdir(image_path)) & set(os.listdir(mask_path))\n",
    "training_files = check\n",
    "\n",
    "def getData(X_shape, flag = \"test\"):\n",
    "    im_array = []\n",
    "    mask_array = []\n",
    "    \n",
    "    if flag == \"test\":\n",
    "        for i in tqdm(testing_files): \n",
    "            im = cv2.resize(cv2.imread(os.path.join(image_path,i)),(X_shape,X_shape))[:,:,0]\n",
    "            mask = cv2.resize(cv2.imread(os.path.join(mask_path,i)),(X_shape,X_shape))[:,:,0]\n",
    "            \n",
    "            im_array.append(im)\n",
    "            mask_array.append(mask)\n",
    "        \n",
    "        return im_array,mask_array\n",
    "    \n",
    "    if flag == \"train\":\n",
    "        for i in tqdm(training_files): \n",
    "            im = cv2.resize(cv2.imread(os.path.join(image_path,i.split(\"_mask\")[0]+\".png\")),(X_shape,X_shape))[:,:,0]\n",
    "            mask = cv2.resize(cv2.imread(os.path.join(mask_path,i+\".png\")),(X_shape,X_shape))[:,:,0]\n",
    "\n",
    "            im_array.append(im)\n",
    "            mask_array.append(mask)\n",
    "\n",
    "        return im_array,mask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform sanity check\n",
    "def plotMask(X,y):\n",
    "    sample = []\n",
    "    \n",
    "    for i in range(6):\n",
    "        left = X[i]\n",
    "        right = y[i]\n",
    "        combined = np.hstack((left,right))\n",
    "        sample.append(combined)\n",
    "        \n",
    "        \n",
    "    for i in range(0,6,3):\n",
    "\n",
    "        plt.figure(figsize=(25,10))\n",
    "        \n",
    "        plt.subplot(2,3,1+i)\n",
    "        plt.imshow(sample[i])\n",
    "        \n",
    "        plt.subplot(2,3,2+i)\n",
    "        plt.imshow(sample[i+1])\n",
    "        \n",
    "        \n",
    "        plt.subplot(2,3,3+i)\n",
    "        plt.imshow(sample[i+2])\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "dim = 256*2\n",
    "X_train,y_train = getData(dim,flag=\"train\")\n",
    "X_test, y_test = getData(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d75cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set\")\n",
    "plotMask(X_train,y_train)\n",
    "print(\"testing set\")\n",
    "plotMask(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685532",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).reshape(len(X_train),dim,dim,1)\n",
    "y_train = np.array(y_train).reshape(len(y_train),dim,dim,1)\n",
    "X_test = np.array(X_test).reshape(len(X_test),dim,dim,1)\n",
    "y_test = np.array(y_test).reshape(len(y_test),dim,dim,1)\n",
    "assert X_train.shape == y_train.shape\n",
    "assert X_test.shape == y_test.shape\n",
    "images = np.concatenate((X_train,X_test),axis=0)\n",
    "mask  = np.concatenate((y_train,y_test),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = keras.flatten(y_true)\n",
    "    y_pred_f = keras.flatten(y_pred)\n",
    "    intersection = keras.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def unet(input_size=(256,256,1)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[conv10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(input_size=(512,512,1))\n",
    "model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss,\n",
    "                  metrics=[dice_coef, 'binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('cxr_reg')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   verbose=1, mode='min', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from keras.optimizers import Adam \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model.compile(optimizer=Adam(lr=2e-4), \n",
    "              loss=[dice_coef_loss], \n",
    "           metrics = [dice_coef, 'binary_accuracy'])\n",
    "\n",
    "train_vol, validation_vol, train_seg, validation_seg = train_test_split((images-127.0)/127.0, \n",
    "                                                            (mask>127).astype(np.float32), \n",
    "                                                            test_size = 0.1,random_state = 2018)\n",
    "\n",
    "train_vol, test_vol, train_seg, test_seg = train_test_split(train_vol,train_seg, \n",
    "                                                            test_size = 0.1, \n",
    "                                                            random_state = 2018)\n",
    "\n",
    "loss_history = model.fit(x = train_vol,\n",
    "                       y = train_seg,\n",
    "                         batch_size = 16,\n",
    "                  epochs = 50,\n",
    "                  validation_data =(test_vol,test_seg) ,\n",
    "                  callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "ax1.plot(loss_history.history['loss'], '-', label = 'Loss')\n",
    "ax1.plot(loss_history.history['val_loss'], '-', label = 'Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(100*np.array(loss_history.history['binary_accuracy']), '-', \n",
    "         label = 'Accuracy')\n",
    "ax2.plot(100*np.array(loss_history.history['val_binary_accuracy']), '-',\n",
    "         label = 'Validation Accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_candidates = np.random.randint(1,validation_vol.shape[0],10)\n",
    "preds = model.predict(validation_vol)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(0,9,3):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    \n",
    "    plt.imshow(np.squeeze(validation_vol[pred_candidates[i]]))\n",
    "    plt.xlabel(\"Base Image\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(3,3,i+2)\n",
    "    plt.imshow(np.squeeze(validation_seg[pred_candidates[i]]))\n",
    "    plt.xlabel(\"Mask\")\n",
    "    \n",
    "    plt.subplot(3,3,i+3)\n",
    "    plt.imshow(np.squeeze(preds[pred_candidates[i]]))\n",
    "    plt.xlabel(\"Pridiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51dce88",
   "metadata": {},
   "source": [
    "# Manually save models weights\n",
    "https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "https://www.tensorflow.org/guide/keras/serialization_and_saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747d962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
